{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# Start capturing video from file\n",
    "cap = cv2.VideoCapture(\"./file.mp4\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # Break the loop when the video ends\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    emotions = []\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI (Region of Interest)\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Perform emotion analysis on the face ROI\n",
    "        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "        # Determine the dominant emotion\n",
    "        emotion = result[0]['dominant_emotion']\n",
    "\n",
    "        # Print the detected emotion\n",
    "        emotions.append(emotion)\n",
    "print(f\"Detected Emotion: {max(set(emotions), key = emotions.count)} {emotions}\")\n",
    "# Release the capture\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start capturing video from file\n",
    "cap = cv2.VideoCapture(\"./file.mp4\")\n",
    "minutes,seconds = 0,0\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read() \n",
    "    if not ret:\n",
    "        break  # Break the loop when the video ends\n",
    "    t_msec = 1000*(minutes*60 + seconds)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, t_msec)  \n",
    "    \n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Initialize a list to store the detected emotions\n",
    "    emotions = []\n",
    "\n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI (Region of Interest)\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Perform emotion analysis on the face ROI\n",
    "        try:\n",
    "            result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "            emotion = result[0]['dominant_emotion']\n",
    "            emotions.append(emotion)\n",
    "        except:\n",
    "            # Ignore any errors that occur during emotion analysis\n",
    "            pass\n",
    "\n",
    "# Print the dominant emotion and the list of detected emotions\n",
    "if emotions:\n",
    "    dominant_emotion = max(set(emotions), key=emotions.count)\n",
    "    print(f\"Detected Emotion: {dominant_emotion} {emotions}\")\n",
    "\n",
    "# Release the capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x224 wig 0.39, shower_cap 0.17, hair_spray 0.03, bow_tie 0.02, neck_brace 0.02, 61.2ms\n",
      "Speed: 71.4ms preprocess, 61.2ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(frame)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Increment count for the predicted class\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop1\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assuming predictions contain class probabilities\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     class_counts[predicted_class] \u001b[38;5;241m=\u001b[39m class_counts\u001b[38;5;241m.\u001b[39mget(predicted_class, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Choose the class with the maximum count\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8n-cls')\n",
    "name=['guru','neelraj','shyam','pradeep','sudeesh']\n",
    "# name[predictions[0].probs.top1]\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read video file\n",
    "# video = cv2.VideoCapture(r\"gru1.mp4\")\n",
    "video = cv2.VideoCapture(r\"./file.mp4\")\n",
    "# video = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize dictionary to store counts for each class\n",
    "class_counts = {}\n",
    "\n",
    "# Read frames from the video\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break  # Break the loop if no frames are read\n",
    "    \n",
    "    # Perform prediction using your YOLO model for the current frame\n",
    "    predictions = model(frame)\n",
    "\n",
    "    # Increment count for the predicted class\n",
    "    predicted_class = name[predictions[0].probs.top1]  # Assuming predictions contain class probabilities\n",
    "    class_counts[predicted_class] = class_counts.get(predicted_class, 0) + 1\n",
    "\n",
    "# Choose the class with the maximum count\n",
    "max_count_class = max(class_counts, key=class_counts.get)\n",
    "print(\"Predicted name:\", max_count_class)\n",
    "\n",
    "# Release video and close OpenCV window\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "model = YOLO('./yolov8n-cls.pt')\n",
    "name=['guru','neelraj','shyam','pradeep','sudeesh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: neutral ['neutral']\n"
     ]
    }
   ],
   "source": [
    "# Start capturing video from file\n",
    "cap = cv2.VideoCapture(\"./file.mp4\")\n",
    "minutes = 0\n",
    "seconds = 0\n",
    "fps = 0.5\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # Break the loop when the video ends\n",
    "    t_msec = 1000*fps*(minutes*60 + seconds)\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, t_msec)\n",
    "    # Convert frame to grayscale\n",
    "    seconds += 1\n",
    "    if seconds > 60:\n",
    "        seconds = 0\n",
    "        minutes += 1\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Initialize a list to store the detected emotions\n",
    "    emotions = []\n",
    "\n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI (Region of Interest)\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Perform emotion analysis on the face ROI\n",
    "        try:\n",
    "            result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "            emotion = result[0]['dominant_emotion']\n",
    "            emotions.append(emotion)\n",
    "        except:\n",
    "            # Ignore any errors that occur during emotion analysis\n",
    "            pass\n",
    "\n",
    "# Print the dominant emotion and the list of detected emotions\n",
    "if emotions:\n",
    "    dominant_emotion = max(set(emotions), key=emotions.count)\n",
    "    print(f\"Detected Emotion: {dominant_emotion} {emotions}\")\n",
    "\n",
    "# Release the capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import warnings\n",
    "\n",
    "# Suppress the warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello, this is testing number one and this is Guru.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.transcribe(\"./recorded-video-2.webm\")\n",
    "result['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
